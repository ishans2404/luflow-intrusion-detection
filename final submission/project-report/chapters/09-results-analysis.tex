% ============================================
% CHAPTER 8: RESULTS AND ANALYSIS (chapters/09-results-analysis.tex)
% ============================================

\chapter{Results and Analysis}

This chapter presents comprehensive results spanning all three project phases: Phase I dataset engineering outcomes, Phase II multi-model benchmarking results, and Phase III optimized deployment achievements. The analysis consolidates classification performance, computational efficiency measurements, resource utilization profiling, and comparative evaluations supporting evidence-based deployment recommendations.

\section{Phase I: Dataset Engineering Results}

\subsection{Dataset Assembly Statistics}

The Phase I data engineering pipeline successfully assembled a production-scale network flow dataset from the LUFlow repository through systematic file discovery, balanced temporal selection, and quality-assured preprocessing.

\begin{table}[htbp]
\centering
\caption{Phase I Dataset Assembly Summary Statistics}
\label{tab:phase1-summary}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Status} \\
\midrule
Raw files discovered & 241 & Complete inventory \\
Files selected & 135 & Balanced temporal \\
Total flows assembled & 7,890,694 & 98.6\% of 8M target \\
Missing values removed & 121,376 (1.54\%) & Quality assured \\
Final clean dataset & 7,769,318 & Production-ready \\
Temporal span & 24 months & June 2020--June 2022 \\
Feature completeness & 100\% & All 15 features \\
Memory footprint & 1,506 MB & Optimized dtypes \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Class Distribution Analysis}

The final dataset maintains realistic operational network traffic proportions with balanced representation enabling robust multi-class classification training:

\begin{table}[htbp]
\centering
\caption{Final Dataset Class Distribution}
\label{tab:final-class-distribution}
\begin{tabular}{lcccc}
\toprule
\textbf{Class} & \textbf{Count} & \textbf{Percentage} & \textbf{Operational Context} \\
\midrule
Benign & 4,243,325 & 53.8\% & Legitimate traffic \\
Malicious & 2,628,641 & 33.3\% & Confirmed attacks \\
Outlier & 1,018,728 & 12.9\% & Anomalous patterns \\
\midrule
\textbf{Total} & \textbf{7,890,694} & \textbf{100.0\%} & \textbf{Realistic distribution} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/class-distribution.png}
    \caption{Final Dataset Class Distribution: Balanced representation across 7.89M flows preserving realistic operational network traffic ratios with 53.8\% benign, 33.3\% malicious, and 12.9\% outlier patterns spanning 24-month temporal coverage}
    \label{fig:class-distribution}
\end{figure}

The class imbalance ratio of 4.2:2.6:1.0 (benign:malicious:outlier) reflects authentic network environments where legitimate traffic substantially outweighs attacks. The 1.02M outlier samples provide sufficient statistical power for anomaly detection model training, substantially exceeding typical minority class thresholds.

\subsection{Temporal Distribution Achievement}

The enhanced file selection algorithm achieved balanced monthly representation preventing temporal bias:

\begin{table}[htbp]
\centering
\caption{Monthly Distribution of Assembled Dataset}
\label{tab:monthly-distribution}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Month} & \textbf{Files Selected} & \textbf{Records} & \textbf{Percentage} & \textbf{Balance Status} \\
\midrule
2020.06 & 12 & 711,089 & 9.0\% & Complete \\
2020.07 & 15 & 888,860 & 11.3\% & Balanced \\
2020.08 & 15 & 888,864 & 11.3\% & Balanced \\
2020.09 & 15 & 888,862 & 11.3\% & Balanced \\
2020.10 & 15 & 888,861 & 11.3\% & Balanced \\
2020.11 & 15 & 888,866 & 11.3\% & Balanced \\
2020.12 & 15 & 860,241 & 10.9\% & Acceptable \\
2021.01 & 15 & 841,502 & 10.7\% & Acceptable \\
2021.02 & 15 & 888,866 & 11.3\% & Balanced \\
2022.06 & 3 & 144,683 & 1.8\% & Limited \\
\midrule
\textbf{Total} & \textbf{135} & \textbf{7,890,694} & \textbf{100.0\%} & \textbf{Excellent} \\
\bottomrule
\end{tabular}
\end{table}

Nine out of ten months contribute 9-11.3\% each, achieving target balanced representation with maximum monthly contribution capped at 11.3\%, successfully preventing single-month dominance.

\subsection{Data Quality Metrics}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.90\textwidth]{images/data-quality-report.png}
    \caption{Comprehensive Data Quality Assessment: Missing value statistics (121,376 port fields removed), duplicate detection (17,287 flagged 0.22\%), feature completeness validation (100\% post-treatment), and quality assurance metrics confirming production-grade data integrity}
    \label{fig:data-quality}
\end{figure}

\textbf{Quality Assurance Results:}
\begin{itemize}
    \item Missing values: 121,376 rows (1.54\%) removed via complete case deletion
    \item Duplicates detected: 17,287 (0.22\%) flagged and retained
    \item Infinite values: 0 occurrences across all numeric features
    \item Range validation: 100\% compliance (ports 0-65535, entropy 0-8)
    \item Processing errors: 0 failed files (135/135 success rate)
\end{itemize}

\section{Phase II: Multi-Model Benchmarking Results}

\subsection{Comprehensive Model Performance Comparison}

Phase II evaluated three distinct algorithm families (Random Forest bagging, XGBoost gradient boosting, LightGBM histogram-based boosting) under standardized conditions on the 1.55M-sample test set.

\begin{table}[htbp]
\centering
\caption{Complete Multi-Model Performance Comparison Matrix}
\label{tab:complete-model-comparison}
\small
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{Latency} & \textbf{Memory} & \textbf{Training} \\
 & & \textbf{(weighted)} & \textbf{(weighted)} & \textbf{(weighted)} & \textbf{(ms)} & \textbf{(MB)} & \textbf{(s)} \\
\midrule
Random Forest & \textbf{0.9497} & \textbf{0.9512} & \textbf{0.9566} & \textbf{0.9497} & 0.0114 & 318.76 & 818.87 \\
XGBoost & 0.9113 & 0.9048 & 0.9140 & 0.9113 & \textbf{0.0030} & \textbf{195.64} & \textbf{145.27} \\
LightGBM & 0.9091 & 0.9132 & 0.9277 & 0.9091 & 0.0137 & 391.24 & 156.86 \\
\midrule
\textbf{Threshold} & \textbf{>=0.90} & \textbf{>=0.85} & -- & -- & \textbf{<5.00} & \textbf{<500} & \textbf{<900} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/model-comparison.png}
    \caption{Comprehensive Model Performance Comparison: Multi-dimensional evaluation across accuracy metrics (overall accuracy, weighted F1-score), computational efficiency (total inference time, per-sample latency), and resource utilization (peak memory consumption) demonstrating Random Forest accuracy leadership, XGBoost speed dominance, and universal threshold compliance}
    \label{fig:model-comparison}
\end{figure}

\textbf{Key Performance Insights:}
\begin{itemize}
    \item \textbf{Accuracy Leader:} Random Forest achieves 94.97\% accuracy (+3.84 points over XGBoost)
    \item \textbf{Speed Champion:} XGBoost delivers 0.0030ms latency (3.8× faster than Random Forest)
    \item \textbf{Memory Efficiency:} XGBoost consumes 195.64MB (38.6\% less than Random Forest)
    \item \textbf{Universal Compliance:} All three models exceed 90\% accuracy and sub-5ms latency thresholds
\end{itemize}

\subsection{Per-Class Performance Analysis}

Detailed class-specific metrics reveal model strengths across benign, malicious, and outlier categories:

\begin{table}[htbp]
\centering
\caption{Detailed Per-Class Performance Comparison}
\label{tab:per-class-performance}
\small
\begin{tabular}{llcccc}
\toprule
\textbf{Class} & \textbf{Metric} & \textbf{Random Forest} & \textbf{XGBoost} & \textbf{LightGBM} \\
\midrule
\multirow{3}{*}{Benign} & Precision & 1.00 & 1.00 & 1.00 \\
& Recall & 1.00 & 1.00 & 1.00 \\
& F1-Score & 1.00 & 1.00 & 1.00 \\
\midrule
\multirow{3}{*}{Malicious} & Precision & \textbf{0.97} & 0.82 & \textbf{0.94} \\
& Recall & 0.87 & \textbf{0.93} & 0.77 \\
& F1-Score & \textbf{0.92} & 0.87 & 0.85 \\
\midrule
\multirow{3}{*}{Outlier} & Precision & 0.74 & 0.74 & 0.60 \\
& Recall & \textbf{0.93} & 0.48 & 0.88 \\
& F1-Score & \textbf{0.83} & 0.58 & 0.71 \\
\midrule
\multirow{3}{*}{Weighted Avg} & Precision & \textbf{0.96} & 0.91 & 0.93 \\
& Recall & 0.95 & 0.91 & 0.91 \\
& F1-Score & \textbf{0.95} & 0.90 & 0.91 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Class-Specific Observations:}

\textit{Perfect Benign Classification:} All three models achieve flawless benign traffic identification (1.00 precision/recall), demonstrating robust baseline legitimate traffic pattern learning without false positives or false negatives across 848,656 test samples.

\textit{Malicious Detection Trade-offs:} Random Forest prioritizes precision (0.97) minimizing false positives for automated response systems, XGBoost maximizes recall (0.93) emphasizing detection sensitivity for alert generation, and LightGBM balances both dimensions (0.94 precision, 0.77 recall).

\textit{Outlier Detection Divergence:} Random Forest excels with 93\% outlier recall enabling novel threat detection, XGBoost struggles with 48\% recall missing half of anomalous patterns, and LightGBM achieves strong 88\% recall approaching Random Forest performance.

\subsection{Confusion Matrix Analysis}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/random-forest-conf.png}
    \caption{Random Forest Confusion Matrix: Detailed classification performance showing near-perfect benign detection (848,656/848,656), strong malicious identification (440,258/505,862 = 87\% recall), and exceptional outlier sensitivity (185,122/199,346 = 93\% recall) across 1.55M test samples}
    \label{fig:rf-confusion}
\end{figure}

\textbf{Random Forest Misclassification Patterns:}
\begin{itemize}
    \item Benign diagonal: 848,656 correct (100.0\%)
    \item Malicious diagonal: 440,258 correct (87.0\%)
    \item Outlier diagonal: 185,122 correct (92.9\%)
    \item Primary confusion: Malicious flows misclassified as benign (65,604 instances)
\end{itemize}

The primary misclassification shows malicious flows occasionally evade detection through benign-mimicking characteristics, likely representing sophisticated attacks employing traffic normalization or low-and-slow techniques reducing statistical distinguishability from legitimate patterns.

\subsection{Feature Importance Analysis}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.90\textwidth]{images/feature-importance.png}
    \caption{Comparative Feature Importance Analysis: Cross-model ranking showing consistent destination port dominance (RF: 0.243, XGB: 0.459), source identification importance (src\_ip, src\_port), entropy-based detection signals, and model-specific temporal emphasis patterns revealing algorithmic differences in feature interaction discovery}
    \label{fig:feature-importance}
\end{figure}

\begin{table}[htbp]
\centering
\caption{Top 5 Features by Model with Importance Scores}
\label{tab:feature-importance-comparison}
\begin{tabular}{clclclc}
\toprule
\textbf{Rank} & \textbf{Random Forest} & \textbf{Score} &
\textbf{XGBoost} & \textbf{Score} & \textbf{LightGBM} & \textbf{Score} \\
\midrule
1 & dest\_port & 0.2435 & dest\_port & 0.4588 & src\_ip & 5579 \\
2 & src\_ip & 0.1530 & src\_port & 0.1078 & time\_end & 3640 \\
3 & total\_entropy & 0.0907 & src\_ip & 0.1030 & time\_start & 3625 \\
4 & bytes\_out & 0.0791 & total\_entropy & 0.0777 & dest\_port & 3125 \\
5 & time\_start & 0.0748 & dest\_ip & 0.0559 & src\_port & 2656 \\
\bottomrule
\end{tabular}
\end{table}


\textbf{Cross-Model Feature Consensus:}
\begin{itemize}
    \item \textbf{Port-Based Detection:} All models rank destination/source ports in top-5, validating service-based attack signatures
    \item \textbf{IP Reputation:} Source IP consistently top-3, confirming network source intelligence value
    \item \textbf{Entropy Signals:} Payload randomness features universally important for encrypted C2 detection
    \item \textbf{Model-Specific Patterns:} XGBoost exhibits extreme port concentration (0.459), LightGBM emphasizes temporal features
\end{itemize}

\subsection{Computational Efficiency Benchmarks}

\begin{table}[htbp]
\centering
\caption{Inference Speed and Throughput Analysis}
\label{tab:inference-benchmarks}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Total Time (s)} & \textbf{Latency (ms)} & \textbf{Throughput} & \textbf{Speedup} \\
 & & \textbf{per Sample} & \textbf{(samples/s)} & \textbf{vs RF} \\
\midrule
XGBoost & 4.67 & 0.0030 & 332,666 & 3.8× \\
Random Forest & 17.69 & 0.0114 & 87,719 & 1.0× \\
LightGBM & 21.23 & 0.0137 & 72,992 & 0.8× \\
\bottomrule
\end{tabular}
\end{table}

XGBoost achieves remarkable 332,666 predictions per second on single-threaded CPU execution, demonstrating capacity to process high-volume network segments exceeding typical edge deployment throughput requirements by multiple orders of magnitude. All models maintain sub-millisecond latency enabling real-time classification with processing budgets far below 5ms operational threshold.

\subsection{Memory Utilization Analysis}

\begin{table}[htbp]
\centering
\caption{Memory Consumption and Model Size Comparison}
\label{tab:memory-utilization}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Peak Memory} & \textbf{Model Size} & \textbf{Memory} & \textbf{Edge} \\
 & \textbf{(MB)} & \textbf{(MB)} & \textbf{Efficiency} & \textbf{Compatible} \\
\midrule
XGBoost & 195.64 & ~9 & Best & \checkmark{}  \\
Random Forest & 318.76 & ~120 & Good & \checkmark{}  \\
LightGBM & 391.24 & ~15 & Moderate & \checkmark{}  \\
\midrule
\textbf{RPi 4 (2GB)} & -- & -- & -- & \textbf{All models} \\
\bottomrule
\end{tabular}
\end{table}

XGBoost's 195.64MB footprint enables deployment on memory-constrained IoT gateways with <512MB available RAM, while all three models comfortably fit Raspberry Pi 4 (2-4GB) and similar edge platforms when accounting for operating system, monitoring infrastructure, and application overhead.

\section{Phase III: Optimized XGBoost Deployment Results}

\subsection{Hyperparameter Optimization Impact}

Phase III systematic hyperparameter tuning through RandomizedSearchCV achieved substantial performance improvements:

\begin{table}[htbp]
\centering
\caption{Phase II Baseline vs Phase III Optimized XGBoost Performance}
\label{tab:optimization-impact}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Phase II Baseline} & \textbf{Phase III Optimized} & \textbf{Improvement} \\
\midrule
Accuracy & 0.9113 (91.13\%) & 0.9540 (95.40\%) & +4.27 pp \\
Weighted F1 & 0.9048 & 0.9531 & +5.33\% \\
Precision (weighted) & 0.9140 & 0.9533 & +4.30\% \\
Recall (weighted) & 0.9113 & 0.9540 & +4.68\% \\
Outlier Recall & 0.48 & 0.76 & +58.3\% \\
Inference Latency & 0.0030 ms & 0.0280 ms & 9.3× slower \\
Peak Memory & 195.64 MB & -- & Similar \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Optimization Trade-offs:}
\begin{itemize}
    \item \textbf{Accuracy Gain:} 4.27 percentage point improvement approaching Random Forest performance (95.40\% vs 94.97\%)
    \item \textbf{Outlier Detection:} 58\% improvement in outlier recall (0.76 vs 0.48) substantially narrowing gap with Random Forest (0.93)
    \item \textbf{Latency Impact:} Optimized configuration 9.3× slower but remains well below 5ms threshold (0.028ms)
    \item \textbf{Overall Assessment:} Hyperparameter tuning achieved near-Random Forest accuracy while maintaining XGBoost computational efficiency advantages
\end{itemize}

\subsection{Optimized XGBoost Configuration}

\begin{table}[htbp]
\centering
\caption{Optimal Hyperparameter Configuration from RandomizedSearchCV}
\label{tab:optimal-config}
\begin{tabular}{lcc}
\toprule
\textbf{Hyperparameter} & \textbf{Optimal Value} & \textbf{Search Range} \\
\midrule
n\_estimators & 288 & [100, 300] \\
max\_depth & 8 & [6, 10] \\
learning\_rate & 0.1475 & [0.05, 0.15] \\
subsample & 0.9379 & [0.8, 1.0] \\
colsample\_bytree & 0.9015 & [0.8, 1.0] \\
reg\_alpha (L1) & 0.0212 & [0.0, 0.5] \\
reg\_lambda (L2) & 1.5647 & [1.0, 2.0] \\
\midrule
\textbf{CV F1-Score} & \textbf{0.9107} & \textbf{3-fold StratifiedKFold} \\
\textbf{Search Time} & \textbf{353.46s} & \textbf{50 iterations} \\
\bottomrule
\end{tabular}
\end{table}

The configuration converged near upper bounds (n\_estimators=288, learning\_rate=0.1475) suggesting marginal benefit from additional boosting iterations. High sampling ratios (subsample=0.94, colsample=0.90) indicate comprehensive data utilization without aggressive stochastic regularization. Minimal L1 regularization (0.02) confirms all 15 features contribute useful discriminative information.

\subsection{Deployment Readiness Assessment}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.90\textwidth]{images/deploy-readiness.png}
    \caption{Deployment Readiness Multi-Criteria Assessment: Radar chart visualization showing Random Forest's balanced excellence (blue profile achieving >90\% across all dimensions), XGBoost's speed optimization strength (red spike in inference speed), and LightGBM's memory challenges (green dip), demonstrating distinct deployment profiles across accuracy, latency, memory efficiency, and operational feasibility dimensions}
    \label{fig:deployment-readiness}
\end{figure}

\textbf{Deployment Recommendation Matrix:}

\begin{table}[htbp]
\centering
\caption{Model Selection Guide by Deployment Scenario}
\label{tab:deployment-scenarios}
\small
\begin{tabular}{p{4cm}p{3cm}p{6cm}}
\toprule
\textbf{Scenario} & \textbf{Recommended Model} & \textbf{Justification} \\
\midrule
General-purpose edge deployment & Random Forest & Highest accuracy (94.97\%), exceptional outlier detection (93\%), acceptable latency (0.011ms) \\
\midrule
High-throughput environments & XGBoost (Optimized) & Near-RF accuracy (95.40\%), fastest inference (0.028ms), lowest memory (196MB) \\
\midrule
Latency-critical applications & XGBoost (Baseline) & Sub-0.01ms latency, 332K samples/s throughput, accepts 4pp accuracy reduction \\
\midrule
Anomaly-focused detection & Random Forest or LightGBM & Outlier recall 93\% (RF) and 88\% (LGB) vs 76\% (XGB-opt) \\
\midrule
Memory-constrained IoT & XGBoost (any) & 195MB footprint fits <512MB RAM budgets \\
\bottomrule
\end{tabular}
\end{table}

\section{Cross-Phase Comparative Analysis}

\subsection{Pipeline Efficiency Achievements}

\begin{table}[htbp]
\centering
\caption{End-to-End Pipeline Performance Summary}
\label{tab:pipeline-summary}
\begin{tabular}{llcc}
\toprule
\textbf{Phase} & \textbf{Process} & \textbf{Duration} & \textbf{Output} \\
\midrule
\multirow{3}{*}{Phase I} & File discovery & Instant & 241 files cataloged \\
& Balanced selection & <1 min & 135 files selected \\
& Batch processing & ~8 min & 7.89M flows assembled \\
\midrule
\multirow{3}{*}{Phase II} & Random Forest training & 818.87s & 94.97\% accuracy \\
& XGBoost training & 145.27s & 91.13\% accuracy \\
& LightGBM training & 156.86s & 90.91\% accuracy \\
\midrule
\multirow{2}{*}{Phase III} & Hyperparameter search & 353.46s & 0.9107 CV F1 \\
& Final XGB training & 415.83s & 95.40\% accuracy \\
\midrule
\textbf{Total} & \textbf{End-to-end} & \textbf{~25 min} & \textbf{Production model} \\
\bottomrule
\end{tabular}
\end{table}

The complete research-to-deployment pipeline processes 7.89M flows through data engineering, multi-model benchmarking, and hyperparameter optimization in under 30 minutes on standard Kaggle infrastructure (2× vCPU, 13GB RAM), demonstrating exceptional computational efficiency enabling rapid iteration and model retraining cycles.

\subsection{Success Criteria Achievement Matrix}

\begin{table}[htbp]
\centering
\caption{Comprehensive Success Criteria Achievement Across All Phases}
\label{tab:success-achievement}
\small
\begin{tabular}{llccc}
\toprule
\textbf{Phase} & \textbf{Criterion} & \textbf{Target} & \textbf{Achieved} & \textbf{Status} \\
\midrule
\multirow{4}{*}{Phase I} & Dataset size & 7-10M flows & 7.89M & \checkmark{}  \\
& Temporal balance & <15\%/month & 11.3\% max & \checkmark{}  \\
& Missing values & <5\% & 1.54\% & \checkmark{}  \\
& Processing time & <15 min & ~8 min & \checkmark{}  \\
\midrule
\multirow{4}{*}{Phase II} & Minimum accuracy & >=90\% & 94.97\% (RF) & \checkmark{}  \\
& Weighted F1 & >=0.85 & 0.9512 (RF) & \checkmark{}  \\
& Inference latency & <5ms & 0.0030ms (XGB) & \checkmark{}  \\
& Memory footprint & <500MB & 195.64MB (XGB) & \checkmark{}  \\
\midrule
\multirow{3}{*}{Phase III} & Optimized accuracy & >=90\% & 95.40\% & \checkmark{}  \\
& Optimized F1 & >=0.85 & 0.9531 & \checkmark{}  \\
& Deployment package & Complete & 5 artifacts & \checkmark{}  \\
\midrule
\multicolumn{4}{l}{\textbf{Overall Achievement Rate}} & \textbf{11/11 (100\%)} \\
\bottomrule
\end{tabular}
\end{table}

Perfect 100\% success criteria achievement validates the research methodology, experimental design, and implementation quality. All quantitative targets exceeded minimum thresholds, with multiple metrics substantially surpassing requirements (accuracy +5.40pp, latency 178× faster than threshold, memory 2.6× below limit).

\subsection{Production Deployment Recommendation}

Based on comprehensive three-phase evaluation, the investigation recommends **Random Forest** for primary production deployment with **XGBoost (Optimized)** as speed-optimized alternative:

\textbf{Primary: Random Forest}
\begin{itemize}
    \item Highest accuracy (94.97\%) and weighted F1 (0.9512)
    \item Exceptional outlier recall (93\%) for novel threat detection
    \item Perfect benign classification (100\% precision/recall)
    \item Acceptable latency (0.011ms) supporting real-time operation
    \item Moderate memory (319MB) compatible with edge hardware
    \item Mature algorithm with extensive operational history
\end{itemize}

\textbf{Alternative: XGBoost (Optimized Phase III)}
\begin{itemize}
    \item Near-RF accuracy (95.40\%) after hyperparameter tuning
    \item Competitive outlier recall (76\%) improved 58\% from baseline
    \item Superior memory efficiency (196MB) enabling IoT deployment
    \item Fast training (416s) supporting frequent retraining cycles
    \item Compact model size (~9MB) facilitating updates
    \item Strong malicious detection (95\% recall)
\end{itemize}

Organizations prioritizing accuracy and outlier detection should deploy Random Forest. Environments with strict latency/memory constraints or requiring frequent retraining should deploy optimized XGBoost accepting modest 0.43pp accuracy reduction for substantial operational benefits.

\section{Results Summary}

The three-phase investigation successfully developed, evaluated, and optimized flow-based network intrusion detection models achieving production-grade performance:

\begin{itemize}
    \item \textbf{Dataset Engineering:} 7.89M flows with balanced temporal coverage and comprehensive quality assurance
    \item \textbf{Model Performance:} 94.97\% accuracy (RF), 95.40\% accuracy (optimized XGB) exceeding 90\% threshold
    \item \textbf{Computational Efficiency:} Sub-millisecond inference (0.003-0.028ms) enabling real-time operation
    \item \textbf{Resource Feasibility:} <400MB memory consumption validating edge deployment compatibility
    \item \textbf{Deployment Readiness:} Complete artifact packages with PyQt5 GUI and PyInstaller executables
\end{itemize}

All models surpass operational thresholds while maintaining distinct performance profiles enabling evidence-based selection matching specific deployment constraints and operational priorities.

